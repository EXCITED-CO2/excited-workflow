{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42973170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e06892",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_data = pd.read_csv(\"site_locations.txt\", \n",
    "                          on_bad_lines='skip', sep = '\\t')\n",
    "\n",
    "#FULLSET_HH is a folder with the HH folders of all the tower data files you want to use (HH==hourly)\n",
    "path_flux = 'FULLSET_HH\\\\'\n",
    "\n",
    "# read all the files with extension .csv\n",
    "filenames = glob.glob(path_flux + \"\\*.csv\")\n",
    "#print('File names:', filenames)\n",
    "\n",
    "data_dict = {}\n",
    "# for loop to iterate all csv files\n",
    "#commented prints are for debugging\n",
    "for file in filenames:\n",
    "    myfilename = file.split('\\\\')[-1].split('.')[0]\n",
    "    myfilename = re.search(\"_(.*?)_\", myfilename).group()\n",
    "    myfilename = myfilename.split('_')[1]\n",
    "    print(\"Reading file = \",myfilename)\n",
    " \n",
    "    temp_df = pd.read_csv(file)\n",
    "\n",
    "    #print(temp_df.head())\n",
    "    #print(latlon_data[latlon_data['Site Id'] == myfilename]['Latitude (degrees)'])\n",
    "    \n",
    "    temp_lat = latlon_data[latlon_data['Site Id'] == myfilename]['Latitude (degrees)'].values[0]\n",
    "    temp_lon = latlon_data[latlon_data['Site Id'] == myfilename]['Longitude (degrees)'].values[0]\n",
    "    \n",
    "    temp_df['latitude'] = temp_lat\n",
    "    temp_df['longitude'] = temp_lon\n",
    "    temp_df['site_name'] = myfilename\n",
    "    \n",
    "    #print(temp_df.head())\n",
    "    \n",
    "    data_dict[myfilename] = temp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict0 = {}\n",
    "for i in data_dict:\n",
    "    print(i)\n",
    "    sub_dict0[i] = data_dict[i][['TIMESTAMP_START', 'NEE_VUT_REF',  'TA_ERA', 'SW_IN_ERA','LW_IN_ERA', 'PA_ERA', 'P_ERA', 'LE_F_MDS', 'H_F_MDS', 'RH']]\n",
    "    sub_dict0[i]['datetime'] = pd.to_datetime(sub_dict0[i]['TIMESTAMP_START'], format = '%Y%m%d%H%M')\n",
    "    \n",
    "sub_combo_df = pd.concat(sub_dict0, axis = 0)\n",
    "sub_combo_df.to_csv('ameriflux_sites_combined_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in data\n",
    "ameri_df = pd.read_csv('ameriflux_sites_combined_subset.csv', index_col = 1, na_values = -9999.0)\n",
    "#naming 'site' column\n",
    "ameri_df.rename(columns = {'Unnamed: 0':'site'}, inplace = True) \n",
    "#removing useless timestamp column\n",
    "timmstmp = ameri_df.pop('TIMESTAMP_START')\n",
    "# mean imputation\n",
    "ameri_df['RH'] = ameri_df['RH'].fillna(ameri_df['RH'].mean())\n",
    "#removing non-NAtemp sites\n",
    "ameri_df = ameri_df[~ameri_df['site'].isin(['US-NGB', 'PE-QFR', 'CA-Lp1', 'AR-TF1'])]\n",
    "#making datetime column datetime typer\n",
    "ameri_df['datetime'] = pd.to_datetime(ameri_df['datetime'], format = '%Y-%m-%d %H:%M:%S')\n",
    "#resetting index\n",
    "ameri_df.reset_index(inplace = True, drop = True)\n",
    "#removing half-hourly entries\n",
    "ameri_df = ameri_df[ameri_df.datetime.dt.minute == 0]\n",
    "#resetting index\n",
    "ameri_df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "reannie_df = ameri_df.copy()\n",
    "reannie_df['yrmon'] = reannie_df['datetime'].dt.strftime('%Y-%m')\n",
    "reannie_df['hour'] = reannie_df['datetime'].dt.strftime('%H')\n",
    "reannie_df\n",
    "\n",
    "rean_L = []\n",
    "\n",
    "print('Beginning loop')\n",
    "for site_i in reannie_df['site'].unique(): #iterating through sites\n",
    "    site_sub = reannie_df[reannie_df['site'] == site_i] # getting a subset of just this site\n",
    "    site_sub = site_sub.drop(['site'], axis = 1) #\n",
    "    \n",
    "    print(site_i)\n",
    "    for yrmon in site_sub['yrmon'].unique():#getting averages for each hour over the month\n",
    "        temp_df = site_sub[site_sub['yrmon'] == yrmon]\n",
    "        reanned = temp_df.groupby(['hour']).mean()\n",
    "        reanned['year-month'] = yrmon\n",
    "        reanned['hour'] = reanned.index\n",
    "        rean_L.append(reanned)\n",
    "\n",
    "\n",
    "mon_hr_avs = pd.concat(rean_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ffc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting column with NEE_VUT meaned 0\n",
    "sub1 = mon_hr_avs[mon_hr_avs['site'] == 'CA-Cbo']\n",
    "sub1.index = pd.to_datetime(sub1['year-month'], format = '%Y-%m')\n",
    "\n",
    "month_av = pd.DataFrame(sub1['NEE_VUT_REF'].resample('1M').mean())\n",
    "month_av['year-month'] = month_av.index\n",
    "month_av['year-month'] = month_av['year-month'].dt.date.apply(lambda x: x.strftime('%Y-%m'))\n",
    "month_av.reset_index(inplace = True, drop = True)\n",
    "\n",
    "sub2 = sub1.reset_index(drop = True)\n",
    "sub2 = sub2.merge(month_av, left_on = 'year-month', right_on = 'year-month')\n",
    "sub2['NEE_VUT_REF_subt'] = sub2['NEE_VUT_REF_x'] - sub2['NEE_VUT_REF_y']\n",
    "\n",
    "mon_hrs_av_0 = sub2\n",
    "mon_hrs_av_0.to_csv('ameri_monthly_reanalysis_by_hour_mean0.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ADS_Thesis] *",
   "language": "python",
   "name": "conda-env-conda-ADS_Thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
