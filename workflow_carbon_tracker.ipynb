{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model on CarbonTracker's carbon flux\n",
    "This notebook outlines the entire workflow to load and preprocess the following data sets, to be able to train a ML model:\n",
    "\n",
    "- CarbonTracker\n",
    "- ERA5 (monthly)\n",
    "- ERA5-land (monthly)\n",
    "- SPEI (monthly)\n",
    "- MODIS (monthly)\n",
    "- Biomass (yearly)\n",
    "\n",
    "First follow the data downloading and config setup instuctions.\n",
    "\n",
    "If you run this notebook on Surf Research Cloud, you shouldn't need to do this anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by setting up a Dask client. This will ensure that Dask can run efficiently to process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdonnelly/excited-workflow/venv/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32819 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import excited_workflow\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import xarray_regrid  # Importing this will make Dataset.regrid accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the CarbonTracker data into an xarray `Dataset` and convert the timestamps (middle point of each month) to a more standard format (1st day of the month), to allow merging with the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cb = xr.open_dataset(\"/data/volume_2/EXCITED_prepped_data/CT2022.flux1x1-monthly.nc\")\n",
    "ds_cb = excited_workflow.utils.convert_timestamps(ds_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other datasets can be found using the excited_workflow.source_datasets module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'biomass': <excited_workflow.source_datasets.biomass.Biomass at 0x7fca4acf5e50>,\n",
       " 'era5_hourly': <excited_workflow.source_datasets.era5.ERA5Hourly at 0x7fca0534f890>,\n",
       " 'era5_monthly': <excited_workflow.source_datasets.era5.ERA5Monthly at 0x7fca0534f350>,\n",
       " 'era5_land_monthly': <excited_workflow.source_datasets.era5.ERA5LandMonthly at 0x7fca0534d310>,\n",
       " 'copernicus_landcover': <excited_workflow.source_datasets.land_cover.LandCover at 0x7fca14a7b190>,\n",
       " 'modis': <excited_workflow.source_datasets.modis.Modis at 0x7fca0534f690>,\n",
       " 'spei': <excited_workflow.source_datasets.spei.Spei at 0x7fca0534f510>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from excited_workflow.source_datasets import datasets\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the desired datasets and merge them into a single xr.Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_data = [\n",
    "    \"biomass\",\n",
    "    \"spei\",\n",
    "    \"modis\",\n",
    "    \"era5_monthly\",\n",
    "    \"era5_land_monthly\",\n",
    "    \"copernicus_landcover\"\n",
    "]\n",
    "ds_input = xr.merge(\n",
    "    [datasets[name].load(freq=\"monthly\", target_grid=ds_cb) for name in desired_data]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the analyis to Transcom region 2 (North America) we require the `regions.nc` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regions = xr.open_dataset(\"/data/volume_2/EXCITED_prepped_data/regions.nc\")\n",
    "# Uncomment the next line to preview the region:\n",
    "#ds_regions[\"transcom_regions\"].where(ds_regions[\"transcom_regions\"]==2).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge everything together. From the CarbonTracker file we only require the `bio_flux_opt` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merged = xr.merge([\n",
    "    ds_cb[[\"bio_flux_opt\"]], \n",
    "    ds_regions[\"transcom_regions\"],\n",
    "    ds_input,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make computations faster and less memory intensive, we can reduce the scope to only North America.\n",
    "\n",
    "This `.sel` operation reduces the size of the dataset from worldwide to only a rectangular area around North America:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_region_na = {\n",
    "    \"time\": slice(\"2010-01\", \"2019-12\"),\n",
    "    \"latitude\": slice(15, 60),\n",
    "    \"longitude\": slice(-140, -55),\n",
    "}\n",
    "ds_na = ds_merged.sel(time_region_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_na = ds_na.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this North American dataset we can mask the transcom region, and preview the 2m air temperature of ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_na = ds_na.where(ds_merged[\"transcom_regions\"]==2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,3))\n",
    "ds_na[\"t2m\"].isel(time=0).plot()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for training, we convert it to a Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds_na.to_dataframe().dropna()\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can our ML models on the data. Here we use pycaret to try a set of models and see which type performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_keys = [\"d2m\", \"mslhf\", \"msshf\", \"ssr\", \"str\", \"t2m\", \"spei\", \"NIRv\", \"skt\", \"stl1\", \"swvl1\", \"lccs_class\"]\n",
    "y_key = \"bio_flux_opt\"\n",
    "\n",
    "df_pycaret = df_train[X_keys + [y_key]]\n",
    "df_reduced = df_pycaret[::10]\n",
    "\n",
    "df_reduced[\"bio_flux_opt\"] = df_reduced[\"bio_flux_opt\"]*1e6  # So RMSE etc. are easier to interpret.\n",
    "\n",
    "import pycaret.regression\n",
    "pycs = pycaret.regression.setup(df_reduced, target=y_key)\n",
    "best = pycs.compare_models(n_select=5, round=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pycaret, the trained models can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycs.plot_model(best[0], plot=\"feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including biomass info, we get a slightly better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_keys = [\"biomass\", \"d2m\", \"mslhf\", \"msshf\", \"ssr\", \"str\", \"t2m\", \"spei\", \"NIRv\", \"skt\", \"stl1\", \"swvl1\"]\n",
    "y_key = \"bio_flux_opt\"\n",
    "\n",
    "df_pycaret = df_train[X_keys + [y_key]]\n",
    "df_reduced = df_pycaret[::10]\n",
    "\n",
    "df_reduced[\"bio_flux_opt\"] = df_reduced[\"bio_flux_opt\"]*1e6  # So RMSE etc. are easier to interpret.\n",
    "\n",
    "import pycaret.regression\n",
    "pycs = pycaret.regression.setup(df_reduced, target=y_key)\n",
    "best = pycs.compare_models(round=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the feature importance plot you can see that the importance of NIRv is now reduced by including biomass info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycs.plot_model(best[0], plot=\"feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this feature imporance depends on the model used. For a different well performing model, the feature imporance is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycs.plot_model(best[3], plot=\"feature\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
