{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model on CarbonTracker's carbon flux\n",
    "This notebook outlines the entire workflow to load and preprocess the ERA5 and CarbonTracker data, to be able to train a ML model.\n",
    "\n",
    "First download the CarbonTracker and monthly ERA5 data using the instructions in the README file.\n",
    "\n",
    "The functions written to process the data are contained in `src/carbontracker.py`. We can import them with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import carbontracker\n",
    "from src import spei\n",
    "from src import utils\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import xarray_regrid  # Importing this will make Dataset.regrid accessible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the CarbonTracker data into an xarray `Dataset` and convert the timestamps (middle point of each month) to a more standard format (1st day of the month), to allow merging with ERA5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"/home/yangliu/Excited/EXCITED_prepped_data\")\n",
    "ds_cb = xr.open_dataset(data_folder / \"CT2022.flux1x1-monthly.nc\")\n",
    "ds_cb = utils.convert_timestamps(ds_cb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load in the monthly ERA5 data. We will have to convert the latitude values to be formatted (-180 -> 180 degrees) instead of (0 -> 360 degrees).\n",
    "\n",
    "Next we coarsen the data to a 1-degree grid, centered around the half values (e.g., [0.5, 1.5, ...])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_era5 = xr.open_mfdataset(\"/home/yangliu/Excited/EXCITED_prepped_data/monthly_era5/*.nc\")\n",
    "ds_era5 = carbontracker.shift_era5_longitude(ds_era5)\n",
    "ds_era5 = carbontracker.coarsen_era5(ds_era5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add SPEI dataset to our recipe. <br>\n",
    "It is also needed to convert the timestamps to a more standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_spei = spei.load_spei_data(data_folder / \"spei/spei06.nc\").sel(time=slice(\"2000-01\", \"2020-12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regrid spei dataset to desired era5 grid\n",
    "ds_spei_regrid = ds_spei.regrid.regrid(ds_era5, method=\"linear\")\n",
    "ds_spei_regrid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the analyis to Transcom region 2 (North America) we require the `regions.nc` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_regions = xr.open_dataset(data_folder / \"regions.nc\")\n",
    "# Uncomment the next line to preview the region:\n",
    "#ds_regions[\"transcom_regions\"].where(ds_regions[\"transcom_regions\"]==2).plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge all three datasets together. From the CarbonTracker file we only require the `bio_flux_opt` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_merged = xr.merge([ds_cb[[\"bio_flux_opt\"]], ds_regions[\"transcom_regions\"], ds_era5, ds_spei])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make computations faster and less memory intensive, we can reduce the scope to only North America.\n",
    "\n",
    "This `.sel` operation reduces the size of the dataset from worldwide to only a rectangular area around North America:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_region_na = {\n",
    "    \"time\": slice(\"2015-01\", \"2020-12\"),\n",
    "    \"latitude\": slice(15, 60),\n",
    "    \"longitude\": slice(-140, -55),\n",
    "}\n",
    "ds_na = ds_merged.sel(time_region_na)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the dataset (instead of leaving it lazy and out-of-memory), as it is small enough to fit into RAM.\n",
    "This operation should not take more than 1 or 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_na = ds_na.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this North American dataset we can mask the transcom region, and preview the 2m air temperature of ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_na = ds_na.where(ds_merged[\"transcom_regions\"]==2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,3))\n",
    "ds_na[\"t2m\"].isel(time=0).plot()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for training, we convert it to a Pandas `DataFrame`.\n",
    "\n",
    "We will remove all rows with NaN values, and unset the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds_na.to_dataframe().dropna().reset_index()\n",
    "df_train.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can our ML models on the data. Here we use pycaret to try a set of models and see which type performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_keys = [\"d2m\", \"mslhf\", \"msshf\", \"ssr\", \"str\", \"t2m\"]\n",
    "y_key = \"bio_flux_opt\"\n",
    "\n",
    "df_pycaret = df_train[X_keys + [y_key]]\n",
    "df_reduced = df_pycaret[::10]\n",
    "\n",
    "df_reduced[\"bio_flux_opt\"] = df_reduced[\"bio_flux_opt\"]*1e6  # So RMSE etc. are easier to interpret.\n",
    "\n",
    "import pycaret.regression\n",
    "pycs = pycaret.regression.setup(df_reduced, target=y_key)# normalize=True, normalize_method=\"robust\")\n",
    "best = pycs.compare_models(round=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "excited",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
